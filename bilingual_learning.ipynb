{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sequence-to-sequence experiment aims to learn joint representation of document in one particular language (e.g. English) to document in another language (e.g. Dutch). For this first stage of experiment, we use a parallel corpus from europarl (the one available in data/europarl_*_test is a splitted sample of first paragraph from the original europarl text document - for a purpose of tutorial).\n",
    "\n",
    "The main code for running on raw europarl data is available in scripts/train_bi_europarl.py (please pay an attention to the folder structure described in this page)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scripts.text_preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n"
     ]
    }
   ],
   "source": [
    "from scripts.language_models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data available here have been preprocessed and the pre-processing code is also available in text_preprocessing.py (the detail of pre-processing stage and tutorial is available in 'preprocessing.ipynb'). \n",
    "\n",
    "Example of pre-processed europarl data, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = readPickle('data/europarl_en_nl_test/vocabulary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vocabulary consists of 2 keys (python dictionary format): \n",
    "\n",
    "[0] for English or 'en' vocabulary, which will be look-up vocabulary/dictionary for the input. \n",
    "\n",
    "[1] for Dutch or 'nl' vocabulary, which is the dictionary for text output in sequence-to-sequence model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'zero'), (1, 'represent'), (2, 'all'), (3, 'plenary'), (4, 'month')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[0].items()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access the word based on index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zero'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the following is vocabulary in Dutch (nl) language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'zero'), (1, 'mensen'), (2, 'doel'), (3, 'Hervatting'), (4, 'inderdaad')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[1].items()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary is not sorted in parallel. So, it is not a translation dictionary, but a look-up indexing words for each language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "461"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data here is a sample from the original document, the number of word English vocabulary is relatively small (461), while the vocabulary of dutch document contains 483 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "483"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = readPickle('data/europarl_en_nl_test/documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['en', 'nl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each language in this sample corpus only consists of a single document. English document consists of total 1197 words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1197"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents['en'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the sequence of words in corresponding document has been encoded into its numerical value based on index in its word vocabulary list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[398, 92, 355, 240, 347]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents['en'][0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To revert back into its text form, you can use indexToWords(vocab,numSentence) from scripts/text_preprocessing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enTxt = indexToWords(vocab[0],documents['en'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Resumption', 'of', 'the', 'session', 'I']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enTxt[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the paralel version - Dutch document consists of 1189 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1189"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents['nl'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 434, 284, 447, 364]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents['nl'][0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlTxt = indexToWords(vocab[1],documents['nl'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hervatting', 'van', 'de', 'zitting', 'Ik']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlTxt[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "File scripts/train_bi_europarl.py can be run on data before preprocessing stage (e.g. directly using europarl data downloaded from corresponding website), but to use this code you need to locate under the following structure of folder:\n",
    "\n",
    "```\n",
    "main_data_path\n",
    "│   \n",
    "└───subfolder (if anys)\n",
    "    │   \n",
    "    └───en\n",
    "    │      europarl.en \n",
    "    │\n",
    "    └───nl\n",
    "           europarl.nl\n",
    "        \n",
    "```\n",
    "And then you can specify the folder location (line 36 - train_bi_europarl.py) as:\n",
    "\n",
    "PATH = 'main_data_path/subfolder'\n",
    "\n",
    "For instance:\n",
    "\n",
    "PATH = 'data/multilingual/europarl'\n",
    "\n",
    "As such the code can generate the python dictionary format of language pair.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other wise, you can use the stored pre-processed data and disregard/comment out line 41 - 47 in main function of train_bi_europarl.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_vocab_len = len(vocab[0])\n",
    "y_vocab_len = len(vocab[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Line 54 split sentences from sequence words of document, but what we got here is encoded version of document (numeric sequence instead of word sequence), but you can use another function in text_preprocessing.py \n",
    "\n",
    "sentToWordsBi()\n",
    "\n",
    "as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "worddocs = sentToWordsBi(documents,vocab)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a document with sequence of words as follow. We will split this tokenized document into array of sentences by using splitSentences() in function getSentencesClass(). Basically, this function splits document into sentences based on characters/punctuations ('.','?','!')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Resumption', 'of', 'the', 'session', 'I']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worddocs['en'][0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, you can just follow the code in scripts/train_bi_europarl.py from line 54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = getSentencesClass(worddocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences[0][0]) # number of sentences in doc-id[0] of language-id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Zero padding...\n"
     ]
    }
   ],
   "source": [
    "numSentences = sentToNumBi(sentences,vocab)\n",
    "nSentences, nWords, minSent, maxSent, sumSent, avgSent, minWords, maxWords, sumWords, avgWords = getStatClass(numSentences)\n",
    "X_max_len = maxWords[0][0]\n",
    "y_max_len = maxWords[1][0]\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "print('[INFO] Zero padding...')\n",
    "\n",
    "X = pad_sequences(numSentences[0][0], maxlen=X_max_len, dtype='int32')\n",
    "y = pad_sequences(numSentences[1][0], maxlen=y_max_len, dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 200\n",
    "HIDDEN_DIM = 200\n",
    "LAYER_NUM = 3\n",
    "BATCH_SIZE = 100\n",
    "NB_EPOCH = 20\n",
    "MODE = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Compiling model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 77, 200)           92200     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 77, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 77, 200)           320800    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 77, 200)           320800    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 77, 200)           320800    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 77, 483)           97083     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 77, 483)           0         \n",
      "=================================================================\n",
      "Total params: 1,472,483\n",
      "Trainable params: 1,472,483\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('[INFO] Compiling model...')\n",
    "model = seqEncoderDecoder(X_vocab_len, X_max_len, y_vocab_len, y_max_len, EMBEDDING_DIM, HIDDEN_DIM, LAYER_NUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll skip the step for checking the stored weight, loading any weights if available, or storing weights when training the model in this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training model: epoch 1th 0/47 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tita/anaconda2/lib/python2.7/site-packages/keras/models.py:834: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "787s - loss: 6.1798 - acc: 0.0014\n",
      "[INFO] Training model: epoch 2th 0/47 samples\n",
      "Epoch 1/1\n",
      "764s - loss: 6.1062 - acc: 0.6789\n",
      "[INFO] Training model: epoch 3th 0/47 samples\n",
      "Epoch 1/1\n",
      "763s - loss: 3.9328 - acc: 0.6789\n",
      "[INFO] Training model: epoch 4th 0/47 samples\n",
      "Epoch 1/1\n",
      "784s - loss: 2.7396 - acc: 0.6789\n",
      "[INFO] Training model: epoch 5th 0/47 samples\n",
      "Epoch 1/1\n",
      "771s - loss: 2.6802 - acc: 0.6789\n",
      "[INFO] Training model: epoch 6th 0/47 samples\n",
      "Epoch 1/1\n",
      "759s - loss: 2.5867 - acc: 0.6789\n",
      "[INFO] Training model: epoch 7th 0/47 samples\n",
      "Epoch 1/1\n",
      "721s - loss: 2.5190 - acc: 0.6789\n",
      "[INFO] Training model: epoch 8th 0/47 samples\n",
      "Epoch 1/1\n",
      "720s - loss: 2.4754 - acc: 0.6789\n",
      "[INFO] Training model: epoch 9th 0/47 samples\n",
      "Epoch 1/1\n",
      "724s - loss: 2.4515 - acc: 0.6789\n",
      "[INFO] Training model: epoch 10th 0/47 samples\n",
      "Epoch 1/1\n",
      "750s - loss: 2.4355 - acc: 0.6789\n",
      "[INFO] Training model: epoch 11th 0/47 samples\n",
      "Epoch 1/1\n",
      "773s - loss: 2.4254 - acc: 0.6789\n",
      "[INFO] Training model: epoch 12th 0/47 samples\n",
      "Epoch 1/1\n",
      "771s - loss: 2.4190 - acc: 0.6789\n",
      "[INFO] Training model: epoch 13th 0/47 samples\n",
      "Epoch 1/1\n",
      "772s - loss: 2.4165 - acc: 0.6789\n",
      "[INFO] Training model: epoch 14th 0/47 samples\n",
      "Epoch 1/1\n",
      "772s - loss: 2.4165 - acc: 0.6789\n",
      "[INFO] Training model: epoch 15th 0/47 samples\n",
      "Epoch 1/1\n",
      "758s - loss: 2.4176 - acc: 0.6789\n",
      "[INFO] Training model: epoch 16th 0/47 samples\n",
      "Epoch 1/1\n",
      "12219s - loss: 2.4153 - acc: 0.6789\n",
      "[INFO] Training model: epoch 17th 0/47 samples\n",
      "Epoch 1/1\n",
      "740s - loss: 2.4104 - acc: 0.6789\n",
      "[INFO] Training model: epoch 18th 0/47 samples\n",
      "Epoch 1/1\n",
      "767s - loss: 2.4024 - acc: 0.6789\n",
      "[INFO] Training model: epoch 19th 0/47 samples\n",
      "Epoch 1/1\n",
      "834s - loss: 2.4123 - acc: 0.6789\n",
      "[INFO] Training model: epoch 20th 0/47 samples\n",
      "Epoch 1/1\n",
      "875s - loss: 2.4017 - acc: 0.6789\n"
     ]
    }
   ],
   "source": [
    "_N = X_max_len\n",
    "_start = 1\n",
    "_end = 0\n",
    "for k in range(_start, NB_EPOCH+1):\n",
    "    xShuffled = shuffleSentences(X)\n",
    "    yShuffled = shuffleSentences(y)\n",
    "    \n",
    "    for i in range(0, len(xShuffled), _N):\n",
    "        if i + _N >= len(xShuffled):\n",
    "            i_end = len(xShuffled)\n",
    "        else:\n",
    "            i_end = i + _N\n",
    "        \n",
    "        yEncoded = sentenceMatrixVectorization(yShuffled[i:i_end], y_max_len, y_vocab_len)\n",
    "        print('[INFO] Training model: epoch {}th {}/{} samples'.format(k, i, len(X)))\n",
    "        model.fit(xShuffled[i:i_end], yEncoded, batch_size=BATCH_SIZE, nb_epoch=1, verbose=2)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
